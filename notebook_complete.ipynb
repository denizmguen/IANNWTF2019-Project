{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Initialization\" data-toc-modified-id=\"Initialization-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Initialization</a></span></li><li><span><a href=\"#Agent\" data-toc-modified-id=\"Agent-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Agent</a></span><ul class=\"toc-item\"><li><span><a href=\"#ReplayBuffer\" data-toc-modified-id=\"ReplayBuffer-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>ReplayBuffer</a></span></li><li><span><a href=\"#Ornstein-Uhlenbeck-Process\" data-toc-modified-id=\"Ornstein-Uhlenbeck-Process-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Ornstein-Uhlenbeck-Process</a></span></li><li><span><a href=\"#DDPG\" data-toc-modified-id=\"DDPG-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>DDPG</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialization\" data-toc-modified-id=\"Initialization-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Initialization</a></span></li><li><span><a href=\"#Updates\" data-toc-modified-id=\"Updates-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Updates</a></span><ul class=\"toc-item\"><li><span><a href=\"#Critic\" data-toc-modified-id=\"Critic-3.3.2.1\"><span class=\"toc-item-num\">3.3.2.1&nbsp;&nbsp;</span>Critic</a></span></li><li><span><a href=\"#Actor\" data-toc-modified-id=\"Actor-3.3.2.2\"><span class=\"toc-item-num\">3.3.2.2&nbsp;&nbsp;</span>Actor</a></span></li></ul></li></ul></li><li><span><a href=\"#Code\" data-toc-modified-id=\"Code-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Code</a></span></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plotting-Trajectories\" data-toc-modified-id=\"Plotting-Trajectories-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Plotting Trajectories</a></span></li></ul></li><li><span><a href=\"#Demo\" data-toc-modified-id=\"Demo-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Demo</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-hardcoded-agent\" data-toc-modified-id=\"Simple-hardcoded-agent-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Simple hardcoded agent</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook contains all the code and explanations that together make up our final project.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the ICLR 2016 (Lillicrap et al., 2016) published a paper called \"Continuous Control with Deep Reinforcement Learning\". In this paper the authors introduce a refined reinforcement learning algorithm named Deep Deterministic Policy Gradient (\"DDPG\"). Their reported results indicate that using the same hyperparamters, the DDPG agent robustly solves various physical control problems with continuous action spaces, outperforming a planning algorithm (Tassa et al., 2012) and DPG (Silver et al., 2014) on whose work DDPG is based. A basic reinforcement learning task which was not included in the report, is the continuous version of OpenAI's Mountain Car. In Mountain Car, an underpowered car has to climb a hill from a valley through leveraging the momentum generated by gravity.\n",
    "\n",
    "<img src=\"img/mountain_climber.jpeg\" height=20% width=20%>\n",
    "\n",
    "Our project is a replication attempt of the 2016 report, focusing on the mountain car problem with a continuous action space. We will begin with writing a tensorflow2/keras implementation of the algorithm and using the exact same parameters as the authors did. Should the agent be inable to solve the problem, we will identify places for improvement and test wether the applied changes will yield a positive effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T09:40:16.791198Z",
     "start_time": "2020-04-03T09:40:10.245387Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym as gym\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float64') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T09:40:17.825172Z",
     "start_time": "2020-04-03T09:40:17.530334Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "COLOR = 'cyan'\n",
    "mpl.rcParams['text.color'] = COLOR\n",
    "mpl.rcParams['axes.labelcolor'] = COLOR\n",
    "mpl.rcParams['xtick.color'] = COLOR\n",
    "mpl.rcParams['ytick.color'] = COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T09:40:19.075815Z",
     "start_time": "2020-04-03T09:40:19.072316Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make sense out of our work, we provide a overview of the algorithm here, taken directly from (Lillicrapt et al. 2016).\n",
    "\n",
    "<img src=\"img/ddpg_algorithm.png\" style=\"height:65%;width:65%;\" align=\"center\">\n",
    "\n",
    "At first glance it is nearly identical to DQN (Mnih et al. 2015). The only major difference lays in the actor update. The implementation of the actor and critic updates in tensorflow 2 will be the core challenge of our project. See <a href=\"#Training\">training</a> for the high level implementation of this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin with the prerequisites for the initialization of our agent. The first one is the replay buffer.\n",
    "\n",
    "<img src=\"img/ddpg_algorithm_rp.png\" style=\"height:50%;width:50%;\" align=\"center\" />\n",
    "\n",
    "As we can see, the replay buffer only needs two functions. The first function 'store' serves to save memory tuples and the second function 'sample_batch' allows us to randomly sample a minibatch of tuples. We took ourselves the freedom to use a nicely optimized class from openai's github repo and made a tiny addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T09:40:21.951513Z",
     "start_time": "2020-04-03T09:40:21.941097Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer for DDPG agents. \n",
    "    Source: 'https://github.com/openai/spinningup/blob/master/spinup/algos/ddpg/ddpg.py'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size, act_dim], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        # Our addition\n",
    "        if len(obs.shape) > 1 or len(next_obs.shape) > 1:\n",
    "            obs = np.reshape(obs, newshape=(-1,2))\n",
    "        #\n",
    "        \n",
    "        self.obs1_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=64):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        return dict(obs1=self.obs1_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs],\n",
    "                    obs2=self.obs2_buf[idxs],\n",
    "                    done=self.done_buf[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ornstein-Uhlenbeck-Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second prerequisite is a noise process to facilitate exploration. \n",
    "\n",
    "<img src=\"img/ddpg_algorithm_ou.png\" style=\"height:50%;width:50%;\" align=\"center\" />\n",
    "\n",
    "The authors chose an Ornstein-Uhlenbeck process to generate the noise required for exploration. According to them, it is a suitable choice for exploration in physical problems with momentum. The random values of the process are temporary correlated. The increment in the random variable $X$ with mean $\\mu$ at time $t$, $dX_t$, is given by the following equation:\n",
    "\n",
    "$$ dX_t = \\theta (\\mu - X_t)dt + \\sigma dW_t $$ \n",
    "\n",
    "where the first summand is a drift term that directs the process towards the mean and the second summad is the differential of a Wiener Process. The differential of a Wiener Process is normally distributed. This means, the second summand is a gaussian with standard deviation $\\sigma$. (Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T09:40:23.602496Z",
     "start_time": "2020-04-03T09:40:23.595553Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeck():\n",
    "    def __init__( self, action_dim=1,x0=None,mu=0.0, sigma=0.2, theta=0.15 ):\n",
    "        self.action_dim = action_dim\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.theta = theta\n",
    "        self.x = x0\n",
    "        self.dt = 1\n",
    "        if x0 is None: self.reset() \n",
    "        \n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Calculates the next value of x depending on the last value of x and the \n",
    "        process' parameters.\n",
    "        \n",
    "        Returns:\n",
    "            x: The next value sampled from a Ornstein Uhlenbeck Process\n",
    "        \"\"\"\n",
    "        \n",
    "        drift = self.theta*(self.mu-self.x)*self.dt\n",
    "        dWt  = np.random.normal(0,1,size=self.action_dim) \n",
    "        dxt = drift + self.sigma*dWt\n",
    "        self.x = self.x + dxt\n",
    "        return self.x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x = np.random.normal(loc=0,scale=self.sigma,size=self.action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we tackle the initialization of our agent.\n",
    "\n",
    "<img src=\"img/ddpg_algorithm_init.png\" height=50% width=50% align=\"center\" />\n",
    "\n",
    "For the actor and critic, we used the parameters for low dimensional problems specified under *Experimental Details* by (Lillicrap et al. 2016). This means our actor as well as our critic have 2 hidden layers with 400 and 300 units. The weights of the hidden layers were initialized from a uniform distribution\n",
    "$[-\\frac{1}{\\sqrt{f_{in}}},\\frac{1}{\\sqrt{f_{in}}}]$. Final layer weights were initialized from a uniform distribution $[-3\\times10^{-3},3\\times10^{-3}]$ (actor),  $[-3\\times10^{-4},3\\times10^{-4}]$ (critic). The critic receives the state as initial input and the action only as input into the second hidden layer. We will use batch normalization on all layers of the actor and all layers prior to the action input in the critic. The initialization for the target networks is trivial as they just receive a copy of the parameters from the actor and critic respectively.\n",
    "\n",
    "Refer to the functions\n",
    "\n",
    "```initialize_actor```, ```initialize_critic```,  ```initialize_target_critic``` and ```initialize_target_actor ``` \n",
    "\n",
    "in <a href=\"#Code\">code</a> for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic\n",
    "Updating the critic parameters is, in this case, synonymous with updating the parameters of the Q-Function. Like in discrete Q-Learning, we must first compute the targets for the Bellman Equation:\n",
    "\n",
    "<img src=\"img/ddpg_algorithm_target2.png\" height=50% width=50% align=\"center\" />\n",
    "\n",
    "We used tensorflow's gradient tape to record the operations that led to the computation of the target. The critical line in which we bring all the components together, is\n",
    "\n",
    "```         \n",
    "critic_targets = rewards + self.GAMMA*(1-dones)*target_critic_out (193).\n",
    "```\n",
    "\n",
    "As a loss function, we used the mean squared error between the target and the output of the critic and an L2 regularizer.\n",
    "\n",
    "<img src=\"img/ddpg_algorithm_cupdate.png\" height=50% width=50% align=\"center\" />\n",
    "\n",
    "Refer to lines (194-210) in ```update_critic``` below for the computation of the error and the update of the critic. \n",
    "\n",
    "#### Actor\n",
    "The update of the actor is based on the DPG algorithm by (Silver et al. 2014). In the image below, $J$ signifies the return (expected long term rewards) if we deterministically follow a policy with parameters $\\theta^{\\mu}$. In the critic update, we computed the gradient with respect to $\\theta^{Q}$ to minimize the loss function. Here, we compute the gradient with respect to $\\theta^{\\mu}$ to maximize the return. \n",
    "<img src=\"img/ddpg_algorithm_aupdate.png\" height=50% width=50% align=\"center\" />\n",
    "\n",
    "Please refer to ```update_actor``` in the cell below for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T09:41:06.910952Z",
     "start_time": "2020-04-03T09:41:06.864802Z"
    },
    "code_folding": [
     110
    ]
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, concatenate, Dense, BatchNormalization\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow import random_uniform_initializer as uniform\n",
    "\n",
    "class DDPGAgent():\n",
    "    ''' \n",
    "    Initiates all subcomponents required for the DDPG algorithm:\n",
    "    Critic Network Q\n",
    "    Actor Network  mu\n",
    "    Target Critic  Q^\n",
    "    Target Actor   mu^\n",
    "    Replay Buffer  R\n",
    "    Noise Process  N\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 env=None,\n",
    "                 actor=None,\n",
    "                 critic=None,\n",
    "                 gamma=0.99,\n",
    "                 tau=0.1,\n",
    "                 batch_size=64,\n",
    "                 replay_buffer_size=int(1e6),\n",
    "                 noise_scale=0.3,\n",
    "                 epsilon=0.999,\n",
    "                 lr_actor=1e-2,\n",
    "                 lr_critic=1e-2):\n",
    "        \n",
    "        # Class Constants\n",
    "        self.ENV = env\n",
    "        self.GAMMA = gamma\n",
    "        self.TAU = tau\n",
    "        self.BATCH_SIZE = batch_size\n",
    "        self.RP_BUFFER_SIZE = replay_buffer_size\n",
    "        self.NOISE_SCALE = noise_scale\n",
    "        self.EPSILON = epsilon # For reducing noise with increasing timesteps \n",
    "        \n",
    "        #Environment Constants for convenience\n",
    "        self.action_dim = env.action_space.shape[0]               \n",
    "        self.observation_dim = env.observation_space.shape[0]  \n",
    "        self.action_high = env.action_space.high\n",
    "        self.action_low = env.action_space.low\n",
    "        \n",
    "        #Replaybuffer\n",
    "        self.rp_buffer = ReplayBuffer(self.observation_dim, self.action_dim,\n",
    "                                      self.RP_BUFFER_SIZE)\n",
    "        #Noise Process\n",
    "        self.noise_process = OrnsteinUhlenbeck(self.action_dim)\n",
    "        \n",
    "        #Networks\n",
    "        self.actor  = self.initialize_actor()\n",
    "        self.critic = self.initialize_critic()\n",
    "        self.target_actor  =  self.initialize_actor()\n",
    "        self.target_critic =  self.initialize_critic()\n",
    "        \n",
    "        #Optimizers\n",
    "        self.optimizer_actor = tf.keras.optimizers.Adam(lr_actor)\n",
    "        self.optimizer_critic = tf.keras.optimizers.Adam(lr_critic)\n",
    "\n",
    "        #For result visualization\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        \n",
    "    #=============================== ACTOR ========================================#\n",
    "    def initialize_actor(self):\n",
    "        ''' Initializes a feedforward neural network that serves as a policy for this agent.'''\n",
    "        \n",
    "        nd1 = 400 # Number of neurons for the first dense layer\n",
    "        nd2 = 300 # Number of neurons for the second dense layer\n",
    "        \n",
    "        # Initializers for different layers\n",
    "        init_h1 = uniform(-1/np.sqrt(self.observation_dim), 1/np.sqrt(self.observation_dim))\n",
    "        init_h2 = uniform(-1/np.sqrt(nd1) , 1/np.sqrt(nd1))\n",
    "        init_out = uniform(-3e-3, 3e-3)\n",
    "        \n",
    "        # Input Layer\n",
    "        x = Input(shape=(self.observation_dim,) ) # Dynamic input layer\n",
    "        x = BatchNormalization(x)\n",
    "        # First Hidden layer\n",
    "        D1 = Dense(nd1,\n",
    "                  activation=None,\n",
    "                  kernel_initializer=init_h1,\n",
    "                  bias_initializer=init_h1)(x)\n",
    "        D1 = BatchNormalization(D1)\n",
    "        D1 = tf.nn.relu(D1)\n",
    "        \n",
    "        # Second Hidden layer\n",
    "        D2 = Dense(nd2,\n",
    "                  activation=None,\n",
    "                  kernel_initializer=init_h2,\n",
    "                  bias_initializer=init_h2)(D1)\n",
    "        D2 = BatchNormalization(D2)\n",
    "        D2 = tf.nn.relu(D2)\n",
    "        \n",
    "        # Output Layer\n",
    "        y = Dense(self.action_dim,\n",
    "                  \"tanh\",\n",
    "                  kernel_initializer=init_out,\n",
    "                  bias_initializer=init_out)(D2) \n",
    "        \n",
    "        \n",
    "        # Model Compilation\n",
    "        actor = Model(inputs=x, outputs=y)\n",
    "        actor.compile(optimizer='adam', loss=\"MSE\") # these are placeholders, we fit the actor manually\n",
    "        \n",
    "        return actor\n",
    "    \n",
    "    def act(self,states,exploration=True):\n",
    "        ''' Compute an action with optional noise\n",
    "        \n",
    "        params:\n",
    "            state: (np.array) Batch of states with shape (batch_size, state_dim)\n",
    "        returns:\n",
    "            action: (np.array) Batch of actions with shape (batch_size, action_dim)\n",
    "        '''  \n",
    "        \n",
    "        # Check wether states have the right shapes should be (0 or batch_size, self.action_dim)\n",
    "        if len(states.shape) == 1:\n",
    "            states = np.expand_dims(states, axis=0)\n",
    "            \n",
    "        if states.shape[1] != self.observation_dim:\n",
    "            states = states.reshape(-1, self.observation_dim)\n",
    "        \n",
    "        action = self.actor(states)\n",
    "        \n",
    "        # Add noise according to ornstein uhlenbeck process with probability EPSILON\n",
    "        if exploration==True and np.random.uniform(0,1) <= self.EPSILON:\n",
    "            noise = self.noise_process.sample()\n",
    "            action += noise\n",
    "\n",
    "        # Clip the action to the bounds given by the environment\n",
    "        action = np.clip(action, self.action_low, self.action_high) \n",
    "        \n",
    "        return action\n",
    "\n",
    "    #=============================== CRITIC =======================================#\n",
    "    def initialize_critic(self):\n",
    "        ''' Initialize a feedforward neural network that serves as the action-value function (or 'Q').''' \n",
    "        nd1 = 400\n",
    "        nd2 = 300\n",
    "        # Initializers for different layers\n",
    "        init_h1 = uniform(-1/np.sqrt(self.observation_dim), 1/np.sqrt(self.observation_dim))\n",
    "        init_h2 = uniform(-1/np.sqrt(nd1) , 1/np.sqrt(nd1))\n",
    "        init_out = uniform(-0.001, 0.001)\n",
    "        \n",
    "        # Input Layer\n",
    "        s = Input(shape=(self.observation_dim,))\n",
    "        s = BatchNormalization(s)\n",
    "        # First Hidden Layer\n",
    "        D1 = Dense(nd1,\n",
    "                  activation=None,\n",
    "                  kernel_initializer=init_h1,\n",
    "                  bias_initializer=init_h1)(s)\n",
    "        D1 = BatchNormalization(D1)\n",
    "        D1 = tf.nn.relu(D1)\n",
    "        \n",
    "        # Second Hidden layer\n",
    "        # Additional Input: Action\n",
    "        a = Input(shape=(self.action_dim,))\n",
    "        D1_a = concatenate([D1,a], axis=1)\n",
    "        \n",
    "        D2= Dense(nd2,\n",
    "                  \"relu\",\n",
    "                  kernel_initializer=init_h2,\n",
    "                  bias_initializer=init_h2)(D1_a)\n",
    "        \n",
    "        # Output Layer\n",
    "        y = Dense(1,\n",
    "                  \"sigmoid\",\n",
    "                  kernel_initializer=init_out,\n",
    "                  bias_initializer=init_out)(D2)\n",
    "        \n",
    "        # Model Compilation\n",
    "        critic = Model(inputs=[s,a], outputs=y)\n",
    "        critic.compile(optimizer=\"adam\", loss=\"MSE\", metrics=[\"mae\"]) # Place Holders, we fit the critic manually\n",
    "        return critic\n",
    "        \n",
    "    #=============================== TARGET NETWORKS ===============================#\n",
    "    def initialize_target_actor(self):\n",
    "        target_actor = self.initialize_actor()\n",
    "        target_actor.set_weights(self.actor.get_weights())\n",
    "        return target_actor\n",
    "    \n",
    "    def initialize_target_critic(self):\n",
    "        target_critic = self.initialize_critic()\n",
    "        target_critic.set_weights(self.critic.get_weights())\n",
    "        return target_critic\n",
    "    \n",
    "    #=============================== UPDATES =======================================#\n",
    "    \n",
    "    def update(self):\n",
    "        ''' Calls the update functions of all networks with a memory batch from the replay buffer. '''\n",
    "        # Get a memory batch from the replay buffer\n",
    "        batch = self.rp_buffer.sample_batch(self.BATCH_SIZE)\n",
    "        states, actions, rewards, states2, dones = batch.values()\n",
    "        \n",
    "        # Convert memory batches (ndarrays) to tensors with 'cvt()' for gradienttape to work. \n",
    "        states = self.cvt(states)\n",
    "        actions = self.cvt(actions)\n",
    "        rewards = self.cvt(rewards)\n",
    "        states2 = self.cvt(states2)\n",
    "        dones = self.cvt(dones)\n",
    "        \n",
    "        # Update the networks\n",
    "        self.update_critic(states, actions, rewards, states2, dones)\n",
    "        self.update_actor(states)\n",
    "        self.update_target_nets()\n",
    "     \n",
    "    def update_critic(self,states, actions, rewards, states2, dones):\n",
    "        with tf.GradientTape() as tp:\n",
    "            ''' The Bellman Update with MSE and L2 regularization'''\n",
    "            # s,a\n",
    "            critic_inputs = [states, actions]\n",
    "            \n",
    "            # Q(s,a) \n",
    "            critic_out = self.critic(critic_inputs)\n",
    "            \n",
    "            # P^(s')            (= a')\n",
    "            target_actions = self.target_actor(states2)\n",
    "            \n",
    "            # Q^(s', P^(s'))   ( = Q^(s',a') )\n",
    "            target_critic_out = self.target_critic([states2, target_actions])\n",
    "            \n",
    "            # y = r + gamma * (1-d) * Q^( s',  P^(s') )               \n",
    "            critic_targets = rewards + self.GAMMA*(1-dones)*target_critic_out\n",
    "            \n",
    "            # MSE = sum( (Q(s,a)-y)Â² )/N \n",
    "            mse = MSE(critic_targets,critic_out)\n",
    "            \n",
    "            # L2 = c*1/2*sum(weights**2)\n",
    "            sum_weights_sq_per_layer = [tf.reduce_sum(v**2) for v in self.critic.trainable_variables \n",
    "                       if \"bias\" not in v.name]\n",
    "            \n",
    "            sum_weights_sq = tf.reduce_sum(sum_weights_sq_per_layer)\n",
    "            \n",
    "            l2 = 0.01 * 1/2 * sum_weights_sq\n",
    "            \n",
    "            # Loss = MSE + L2 \n",
    "            critic_loss = mse + l2\n",
    "            self.critic_losses.append(critic_loss)\n",
    "            \n",
    "            # Apply Gradients\n",
    "            critic_gradients = tp.gradient(critic_loss, self.critic.trainable_variables)\n",
    "            self.optimizer_critic.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))  \n",
    "            \n",
    "        del tp\n",
    "        \n",
    "    def update_actor(self, states):\n",
    "        ''' Updates the parameters of the actor network '''\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tp: # Has to be persistent since we compute multiple gradients\n",
    "            # P(s)      64 Values\n",
    "            actions = self.actor(states)\n",
    "            \n",
    "            # Q(s,P(s)) 64 Values\n",
    "            Qs = self.critic([states,actions])\n",
    "            \n",
    "            # Compute the individual gradient vectors for each Q\n",
    "            # list 64 gradients, each gradient is a len-6-list: [(2,400),(400),(400,300),(300),(300,1),(1)],\n",
    "            # because our actor has 3 layers and 3 biases.\n",
    "            gradients = [tp.gradient(Q, self.actor.trainable_variables) for Q in Qs[:3]]\n",
    "            shapes1 = [gradient.shape for gradient in gradients[0]]\n",
    "            shapes2 = [gradient.shape for gradient in gradients[1]]\n",
    "            \n",
    "            logging.debug(f'''gradients has self.batch_size entries:\n",
    "                              {len(gradients) == self.BATCH_SIZE},\n",
    "                              gradients[0] has as many entries as there are layers and biases in actor:\n",
    "                              {len(gradients[0]) == len(self.actor.trainable_variables)}\n",
    "                            '''\n",
    "                         )\n",
    "            \n",
    "            # Neither a list nor a numpy array of tensors with different shapes is convertable\n",
    "            # to a tensor by tensorflow, which is why we cant simply call tf_reduce_mean() on gradients.\n",
    "            # Therefore, we will compute the mean of the gradients manually in\n",
    "            # self.mean_of_gradients()\n",
    "            \n",
    "                              \n",
    "        # Compute the mean gradient\n",
    "        mean_actor_gradient = self.mean_of_gradients(gradients)\n",
    "        logging.debug(f'''Mean_actor gradient should have the following properties:\n",
    "                        len == len(actor tv) : {len(mean_actor_gradient) == len(self.actor.trainable_variables)}\n",
    "                        shape of G1 == shape of D1: {mean_actor_gradient[0].shape == gradients[0][0].shape}\n",
    "                        No Nones : {not None in mean_actor_gradient[0].numpy()}\n",
    "                        ''')\n",
    "        \n",
    "        # Apply Gradient\n",
    "        self.optimizer_actor.apply_gradients(zip(mean_actor_gradient, self.actor.trainable_variables))    \n",
    "        \n",
    "        del tp\n",
    "        \n",
    "    def update_target_nets(self):\n",
    "        aw = agent.actor.get_weights()           #actor weights\n",
    "        cw = agent.critic.get_weights()          #critic weights\n",
    "        taw = agent.target_actor.get_weights()   #target actor weights\n",
    "        tcw = agent.target_critic.get_weights()  #target critic weights\n",
    "        \n",
    "        ntaw = [agent.TAU*w + (1-agent.TAU)*tw for w, tw in zip(aw,taw)] #new target actor weights\n",
    "        agent.target_actor.set_weights(ntaw)\n",
    "        \n",
    "        ntcw = [agent.TAU*w + (1-agent.TAU)*tw for w, tw in zip(cw,tcw)] #new target critic weights\n",
    "        agent.target_critic.set_weights(ntcw)\n",
    "\n",
    "    #== Utils ==#\n",
    "    def remember(self, state_transitions, gamma=0.98):\n",
    "        ''' \n",
    "        Function to store a list of state_transitions\n",
    "        '''\n",
    "        # state_transitions[i] = [s_i,a_i,r_i,s'_i,d_i] \n",
    "        # 0:state, 1:action, 2:reward, 3:next_state, 4:done\n",
    "        if type(state_transitions[0]) != list:\n",
    "            state_transitions = [state_transitions]\n",
    "            \n",
    "        # Modify the rewards of the trajectory (say, the last 100 steps) that lead to the goal state\n",
    "        '''n_sts = len(state_transitions)\n",
    "        if state_transitions[-1][2] > 50:\n",
    "            for i in range(2,min(50, n_sts)):\n",
    "                state_transitions[-i][2] += state_transitions[-i+1][2] * gamma\n",
    "            # We want to overrepresent goal trajectories in the memory batch such that\n",
    "            # the probability that they are learned is increased.\n",
    "            state_transitions *= 10'''\n",
    "            \n",
    "        # Add State transitions to replay buffer\n",
    "        for state,action,reward,next_state,done in state_transitions:\n",
    "            agent.rp_buffer.store(state.reshape(-1,self.observation_dim),\n",
    "                                  action,\n",
    "                                  reward,\n",
    "                                  next_state.reshape(-1,self.observation_dim),\n",
    "                                  done)\n",
    "\n",
    "    def cvt(self,x):\n",
    "        '''quickly converts arrays to tensors'''\n",
    "        return tf.convert_to_tensor(x, dtype=tf.float64)\n",
    "    \n",
    "    def mean_of_gradients(self,gradients:list):\n",
    "        ''' Takes a list of of gradient lists and returns a list with the mean of all gradients'''\n",
    "        \n",
    "        nlayers = len(gradients[0])\n",
    "        mean_gradients = list(np.zeros(nlayers)) # initialize a list with nlayers entries\n",
    "        \n",
    "        #For each layer i\n",
    "        for i in range(nlayers):\n",
    "            sum_i = 0\n",
    "            #For all gradients that have been computed for this layer:\n",
    "            for j in range(len(gradients)):\n",
    "                sum_i += gradients[j][i]\n",
    "            #Compute the average gradient for the ith layer\n",
    "            avg_i = sum_i / len(gradients)\n",
    "            mean_gradients[i] = avg_i\n",
    "        \n",
    "        return mean_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T09:41:07.861577Z",
     "start_time": "2020-04-03T09:41:07.856617Z"
    }
   },
   "outputs": [],
   "source": [
    "ENV = gym.make(\"MountainCarContinuous-v0\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T09:41:08.546175Z",
     "start_time": "2020-04-03T09:41:08.505973Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "axis must be int or list, type given: <class 'tensorflow.python.framework.ops.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2374d0c4a50e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDDPGAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mENV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-b19783a46049>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, env, actor, critic, gamma, tau, batch_size, replay_buffer_size, noise_scale, epsilon, lr_actor, lr_critic)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m#Networks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_actor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_critic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_actor\u001b[0m  \u001b[1;33m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_actor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-b19783a46049>\u001b[0m in \u001b[0;36minitialize_actor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;31m# Input Layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;31m# Dynamic input layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;31m# First Hidden layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         D1 = Dense(nd1,\n",
      "\u001b[1;32mc:\\users\\host\\programming\\miniconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\normalization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, axis, momentum, epsilon, center, scale, beta_initializer, gamma_initializer, moving_mean_initializer, moving_variance_initializer, beta_regularizer, gamma_regularizer, beta_constraint, gamma_constraint, renorm, renorm_clipping, renorm_momentum, fused, trainable, virtual_batch_size, adjustment, name, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m       raise TypeError('axis must be int or list, type given: %s'\n\u001b[1;32m--> 169\u001b[1;33m                       % type(axis))\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: axis must be int or list, type given: <class 'tensorflow.python.framework.ops.Tensor'>"
     ]
    }
   ],
   "source": [
    "agent = DDPGAgent(ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T09:40:31.643426Z",
     "start_time": "2020-04-03T09:40:31.617609Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Episode 0 out of 3\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-83abd14c4e29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mENV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoise_process\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTIMESTEPS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "EPISODES = 3\n",
    "TIMESTEPS = 999\n",
    "RENDER = False\n",
    "TRAIN_INTERVAL = 1\n",
    "VERBOSE = True\n",
    "\n",
    "episodic_rewards = []\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    logging.info(f'Episode {e} out of {EPISODES}')\n",
    "\n",
    "    rewards = []\n",
    "    state_transitions = []\n",
    "    \n",
    "    state = ENV.reset()\n",
    "    agent.noise_process.reset()\n",
    "\n",
    "    for t in range(TIMESTEPS):\n",
    "        # Act\n",
    "        action = agent.act(state, exploration=True)\n",
    "\n",
    "        # Observe\n",
    "        next_state, reward, done, _ = ENV.step(action)\n",
    "        if RENDER: ENV.render()\n",
    "\n",
    "        # Remember \n",
    "        agent.remember([state,action,reward,next_state,done])\n",
    "        \n",
    "        # Update\n",
    "        if t % TRAIN_INTERVAL == 0:\n",
    "            if len(agent.rp_buffer.obs1_buf) >= agent.BATCH_SIZE:\n",
    "                agent.update()\n",
    "        \n",
    "        \n",
    "        # (Update State)\n",
    "        state = next_state\n",
    "        # (Record Performance)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        # End\n",
    "        if done or t == TIMESTEPS-1:\n",
    "            # Save Episode Score\n",
    "            episodic_reward = sum(rewards)\n",
    "            episodic_rewards.append(episodic_reward)\n",
    "            # Print Info\n",
    "            if VERBOSE:\n",
    "                info = \" in a goal state \" if done else \" \"\n",
    "                logging.info(f'Episode ended{info}with cumulative reward of:{episodic_reward}')\n",
    "\n",
    "            \n",
    "            break\n",
    "            \n",
    "ENV.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:46:54.598481Z",
     "start_time": "2020-04-02T10:46:54.558481Z"
    }
   },
   "outputs": [],
   "source": [
    "ENV.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T16:44:06.131550Z",
     "start_time": "2020-03-21T16:44:06.108588Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5), dpi=100)\n",
    "plt.plot(episodic_rewards, lw=0.7, label=\"raw\")\n",
    "plt.gca().set(title=\"Learning Progress of DDPG Agent\",\n",
    "              xlabel=\"timesteps\",\n",
    "              ylabel=\"reward\")\n",
    "plt.grid()\n",
    "#plt.plot(pd.Series(np.array(rewards)).rolling(100,center=True).mean(), lw=1.5,label=\"smoothed\")\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T20:17:43.630235Z",
     "start_time": "2020-03-16T20:17:43.250218Z"
    }
   },
   "outputs": [],
   "source": [
    "titles = {0: \"States\", 1: \"Actions\", 2: \"Rewards\", 3: \"Next States\"}\n",
    "asts = np.asarray(state_transitions[-200:])\n",
    "fig, axes = plt.subplots(2, 1, figsize=(5, 10), dpi=100)\n",
    "\n",
    "for i in [1, 2]:\n",
    "    axes[i - 1].plot(asts[:, i])\n",
    "    axes[i - 1].set(title=titles[i])\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:50:43.383079Z",
     "start_time": "2020-04-02T10:50:25.733049Z"
    }
   },
   "outputs": [],
   "source": [
    "E = 1\n",
    "T = 1000\n",
    "for e in range(E):\n",
    "    state = ENV.reset()\n",
    "    for t in range(T):\n",
    "        a = agent.act(state, exploration=False)\n",
    "        s,a,r,d = ENV.step(a)\n",
    "        ENV.render()\n",
    "        if d:\n",
    "            break\n",
    "ENV.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T19:07:01.578578Z",
     "start_time": "2020-03-05T19:07:01.575602Z"
    }
   },
   "source": [
    "## Simple hardcoded agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:51:41.188535Z",
     "start_time": "2020-04-02T10:51:26.033701Z"
    }
   },
   "outputs": [],
   "source": [
    "E = 1\n",
    "T = 900\n",
    "states = np.zeros(T * 2).reshape(T, 2)\n",
    "for e in range(E):\n",
    "    s = ENV.reset()\n",
    "    for t in range(T):\n",
    "        acc = -0.2 if s[1] < 0  else 0.2\n",
    "        a = np.array([acc])\n",
    "        s, r, i, d = ENV.step(a)\n",
    "        states[t] = s\n",
    "        ENV.render()\n",
    "        if d:\n",
    "            break\n",
    "ENV.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:51:43.934107Z",
     "start_time": "2020-04-02T10:51:43.754867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a50389c748>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(states[:, 0], label=\"Position\")\n",
    "plt.plot(states[:, 1], label=\"Velocity\")\n",
    "plt.ylabel(\"values\")\n",
    "plt.xlabel(\"timesteps\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "47.2px",
    "left": "46px",
    "top": "140.4px",
    "width": "271px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 447.02822199999997,
   "position": {
    "height": "469.15px",
    "left": "895.338px",
    "right": "20px",
    "top": "0px",
    "width": "498.367px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
