{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Initialization\" data-toc-modified-id=\"Initialization-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Initialization</a></span></li><li><span><a href=\"#Agent\" data-toc-modified-id=\"Agent-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Agent</a></span><ul class=\"toc-item\"><li><span><a href=\"#ReplayBuffer\" data-toc-modified-id=\"ReplayBuffer-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>ReplayBuffer</a></span></li><li><span><a href=\"#DDPG-Class\" data-toc-modified-id=\"DDPG-Class-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>DDPG Class</a></span></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Plotting-Results\" data-toc-modified-id=\"Plotting-Results-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Plotting Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Agent-Losses\" data-toc-modified-id=\"Agent-Losses-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Agent Losses</a></span><ul class=\"toc-item\"><li><span><a href=\"#Target-Function\" data-toc-modified-id=\"Target-Function-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Target Function</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "*This notebook contains all the code and explanations that together make up our final project.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:29:13.578820Z",
     "start_time": "2020-01-22T11:29:12.926199Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T12:00:16.570139Z",
     "start_time": "2020-01-22T12:00:16.567158Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:29:27.863915Z",
     "start_time": "2020-01-22T11:29:27.854953Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer for DDPG agents. \n",
    "    'https://github.com/openai/spinningup/blob/master/spinup/algos/ddpg/ddpg.py'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size, act_dim], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs1_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=64):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        return dict(obs1=self.obs1_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs],\n",
    "                    obs2=self.obs2_buf[idxs],\n",
    "                    done=self.done_buf[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## DDPG Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T12:11:11.602964Z",
     "start_time": "2020-01-22T12:11:11.572008Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, concatenate, Dense\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MSE\n",
    "\n",
    "class DDPGAgent():\n",
    "    def __init__(self, env=None, actor=None, critic=None, gamma=0.99,\n",
    "                 tau=0.005,batch_size=64,replay_buffer_size=int(10e6), noise_scale=0.3,epsilon=0.9):\n",
    "        # Class Constants\n",
    "        self.ENV = env\n",
    "        self.GAMMA = gamma\n",
    "        self.TAU = tau\n",
    "        self.BATCH_SIZE = batch_size\n",
    "        self.RP_BUFFER_SIZE = replay_buffer_size\n",
    "        self.NOISE_SCALE = noise_scale\n",
    "        self.EPSILON = epsilon # \n",
    "        \n",
    "        #Just for convenience\n",
    "        self.action_dim = env.action_space.shape[0]               \n",
    "        self.observation_dim = env.observation_space.shape[0]  \n",
    "        self.action_high = env.action_space.high\n",
    "        self.action_low = env.action_space.low\n",
    "        \n",
    "        #Replaybuffer\n",
    "        self.rp_buffer = ReplayBuffer(self.observation_dim, self.action_dim,\n",
    "                                      self.RP_BUFFER_SIZE)\n",
    "        # Networks\n",
    "        self.actor  = self.initialize_actor()\n",
    "        self.critic = self.initialize_critic()\n",
    "        self.target_actor  =  self.initialize_actor()\n",
    "        self.target_critic =  self.initialize_critic()\n",
    "        self.optimizer = optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        # Dummy Inputs for actor loss\n",
    "        self.dummy_Q_target_prediction_input = np.zeros((self.BATCH_SIZE, 1))\n",
    "        self.dummy_dones_input = np.zeros((self.BATCH_SIZE, 1))\n",
    "        \n",
    "        #Progress Tracking\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        \n",
    "        \n",
    "    #=============================== ACTOR ===============================#\n",
    "    \n",
    "    def initialize_actor(self):\n",
    "        x = Input(shape=(self.observation_dim,))\n",
    "        D1= Dense(32,\"relu\",\n",
    "                    kernel_initializer='he_uniform',\n",
    "                    bias_initializer=\"he_uniform\")(x)\n",
    "        D2= Dense(32,\"relu\",\n",
    "                    kernel_initializer='he_uniform',\n",
    "                    bias_initializer=\"he_uniform\")(D1)\n",
    "        \n",
    "        y = Dense(1,\"sigmoid\",\n",
    "                    kernel_initializer=\"he_uniform\",\n",
    "                    bias_initializer=\"he_uniform\")(D2)\n",
    "        \n",
    "        actor = Model(inputs=x, outputs=y)\n",
    "        actor.compile(optimizer='adam', loss=\"MSE\") # does not matter anyway\n",
    "        return actor\n",
    "    \n",
    "\n",
    "    def act(self,states,noise=None):\n",
    "        \"\"\"Returns an action (=prediction of local actor) given a state.\n",
    "        Adds a gaussion noise for exploration. \n",
    "        params:\n",
    "            :state: the state batch\n",
    "            :noise: add noise. If None defaults self.ACT_NOISE_SCALE is used.\n",
    "                    If 0 ist passed, no noise is added and clipping passed\n",
    "        \"\"\"   \n",
    "        if len(states.shape) == 1: states = states.reshape(1,-1)   \n",
    "        noise = np.random.normal(0,self.NOISE_SCALE, len(states))\n",
    "        #states = self.cvt(states)\n",
    "        action = self.actor(states)\n",
    "        action += noise\n",
    "        action = np.clip(action, self.action_low, self.action_high)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    \n",
    "    #=============================== CRITIC ===============================#\n",
    "    \n",
    "    def initialize_critic(self):\n",
    "        s = Input(shape=(self.observation_dim,))\n",
    "        a = Input(shape=(self.action_dim,))\n",
    "        x = concatenate([s,a], axis=1)\n",
    "        D1= Dense(32,\"relu\",\n",
    "                    kernel_initializer='he_uniform',\n",
    "                    bias_initializer=\"he_uniform\")(x)\n",
    "        D2= Dense(32,\"relu\",\n",
    "                    kernel_initializer='he_uniform',\n",
    "                    bias_initializer=\"he_uniform\")(D1)\n",
    "        \n",
    "        y = Dense(1,\"sigmoid\",\n",
    "                    kernel_initializer=\"he_uniform\",\n",
    "                    bias_initializer=\"he_uniform\")(D2)\n",
    "        \n",
    "        critic = Model(inputs=[s,a], outputs=y)\n",
    "        critic.compile(optimizer=\"adam\", loss=\"MSE\", metrics=[\"mae\"]) # Just for show, we update manually\n",
    "        return critic\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #=============================== TARGET NETWORKS ===============================#\n",
    "    def initialize_target_actor(self):\n",
    "        target_actor = self.initialize_actor()\n",
    "        target_actor.set_weights(self.actor.get_weights())\n",
    "        return target_actor\n",
    "    \n",
    "    def initialize_target_critic(self):\n",
    "        target_critic = self.initialize_critic()\n",
    "        target_critic.set_weights(self.critic.get_weights())\n",
    "        return target_critic\n",
    "    \n",
    "    \n",
    "    \n",
    "    #=============================== UPDATES ===============================#\n",
    "    \n",
    "    # Updates all subcomponents of the actor\n",
    "    def update(self):\n",
    "        batch = self.rp_buffer.sample_batch(self.BATCH_SIZE)\n",
    "        states, actions, rewards, states2, dones = batch.values()\n",
    "        \n",
    "        # Convert memories to tensors 'cvt()' for gradienttape to work\n",
    "        states = self.cvt(states)\n",
    "        actions = self.cvt(actions)\n",
    "        rewards = self.cvt(rewards)\n",
    "        states2 = self.cvt(states2)\n",
    "        dones = self.cvt(dones)\n",
    "\n",
    "        self.update_critic(states, actions, rewards, states2, dones)\n",
    "        self.update_actor(states)\n",
    "        self.update_target_nets()\n",
    "     \n",
    "    \n",
    "    def update_critic(self,states, actions, rewards, states2, dones):\n",
    "        with tf.GradientTape() as tp:\n",
    "            ''' The Bellman Update with MSE '''\n",
    "            # s,a\n",
    "            critic_inputs = [states, actions]\n",
    "            \n",
    "            # Q(s,a) \n",
    "            critic_out = self.critic(critic_inputs)\n",
    "            \n",
    "            # P^(s')\n",
    "            target_actions = self.target_actor(states2)\n",
    "            \n",
    "            # Q^(s', P^(s'))\n",
    "            target_critic_out = self.target_critic([states2, target_actions])\n",
    "            \n",
    "            # y = r + gamma * (1-d) * Q^( s',  P^(s') )               \n",
    "            critic_targets = rewards + self.GAMMA*(1-dones)*target_critic_out\n",
    "            \n",
    "            # L2 = sum( (Q(s,a)-y)² ) / Batchsize\n",
    "            critic_loss = MSE(critic_targets,critic_out)\n",
    "            self.critic_losses.append(critic_loss)\n",
    "            \n",
    "            critic_gradients = tp.gradient(critic_loss, self.critic.trainable_variables)\n",
    "            \n",
    "            self.optimizer.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))  \n",
    "        \n",
    "    def update_actor(self, states):\n",
    "        with tf.GradientTape() as tp:\n",
    "            ''' Gradient Update for the Policy in Continuous Action Space. '''\n",
    "            # P(s)\n",
    "            actions = self.actor(states)\n",
    "            \n",
    "            # Q(s,P(s))\n",
    "            actor_target = self.critic([states,actions])\n",
    "            \n",
    "            # -mean(Q(s,P(s))) negative because Adam minimizes the loss\n",
    "            # We want to maximize actor_target\n",
    "            actor_target = -tf.reduce_mean(actor_target)\n",
    "            self.actor_losses.append(actor_target)\n",
    "            \n",
    "            actor_gradients = tp.gradient(actor_target, self.actor.trainable_variables)\n",
    "            \n",
    "            self.optimizer.apply_gradients(zip(actor_gradients, self.actor.trainable_variables))\n",
    "        \n",
    "    \n",
    "    \n",
    "    def update_target_nets(self):\n",
    "        aw = agent.actor.get_weights()\n",
    "        cw = agent.critic.get_weights()\n",
    "        taw = agent.target_actor.get_weights()\n",
    "        tcw = agent.target_critic.get_weights()\n",
    "\n",
    "        naw = [agent.TAU*w + (1-agent.TAU)*tw for w, tw in zip(aw,taw)]\n",
    "        agent.target_actor.set_weights(naw)\n",
    "        \n",
    "        ncw = [agent.TAU*w + (1-agent.TAU)*tw for w, tw in zip(cw,tcw)]\n",
    "        agent.target_critic.set_weights(ncw)\n",
    "        \n",
    "    #== Utils ==#\n",
    "    def cvt(self,x):\n",
    "        '''quickly converts arrays to tensors'''\n",
    "        return tf.convert_to_tensor(x, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T12:11:40.122985Z",
     "start_time": "2020-01-22T12:11:39.859456Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Initialize the environment here: cartpole/mountain_climber ..\n",
    "# env = \n",
    "agent = DDPGAgent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The loop below should perhaps be a function within the agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T12:27:06.667209Z",
     "start_time": "2020-01-22T12:18:54.672830Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_log = []\n",
    "action_log = []\n",
    "reward_log = []\n",
    "\n",
    "EPISODES = 5\n",
    "TIMESTEPS = 100000\n",
    "\n",
    "state,_,_,_ = env.reset()\n",
    "for e in range(EPISODES):\n",
    "    state,_,_,_ = env.reset()\n",
    "    \n",
    "    training_log = []\n",
    "    train_interval = 100\n",
    "    \n",
    "    for t in range(TIMESTEPS):\n",
    "        # Act\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Observe\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Remember\n",
    "        agent.rp_buffer.store(state,action,reward,next_state,done)\n",
    "        \n",
    "        # Learn\n",
    "        if t % train_interval == 0:\n",
    "            agent.update() \n",
    "        \n",
    "        # Record Data\n",
    "        state_log.append(state)\n",
    "        action_log.append(action[0][0])\n",
    "        reward_log.append(reward)\n",
    "        \n",
    "        # Update State\n",
    "        state = next_state\n",
    "        \n",
    "        # End\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "# Store Data\n",
    "training_data = pd.DataFrame({\"state\":state_log, \"action\": action_log, \"reward\":reward_log})\n",
    "state_data = np.array(state_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Plotting Results"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 447.05999999999995,
   "position": {
    "height": "40px",
    "left": "843.352px",
    "right": "20px",
    "top": "0px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
